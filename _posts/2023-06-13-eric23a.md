---
title: Do We Still Need Clinical Language Models?
abstract: Although recent advances in scaling large language models (LLMs) have resulted
  in improvements on many NLP tasks, it remains unclear whether these models trained
  primarily with general web text are the right tool in highly specialized, safety
  critical domains such as \emph{clinical text}. Recent results have suggested that
  LLMs encode a surprising amount of medical knowledge.  This raises an important
  question regarding the utility of smaller domain-specific language models. With
  the success of general-domain LLMs, is there still a need for specialized clinical
  models?  To investigate this question, we conduct an extensive empirical analysis
  of 12 language models, ranging from 220M to 175B parameters, measuring their performance
  on 3 different clinical tasks that test their ability to parse and reason over electronic
  health records. As part of our experiments, we train T5-Base and T5-Large models
  from scratch on clinical notes from MIMIC III and IV to directly investigate the
  efficiency of clinical tokens. We show that relatively small specialized clinical
  models substantially outperform all in-context learning approaches, even when finetuned
  on limited annotated data.  Further, we find that pretraining on clinical tokens
  allows for smaller, more parameter-efficient models that either match or outperform
  much larger language models trained on general text.  We release the code and the
  models used under the PhysioNet Credentialed Health Data license and data use agreement.\footnote{\href{https://github.com/elehman16/clinical_llm}{https://github.com/elehman16/clinical_llm}}
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eric23a
month: 0
tex_title: Do We Still Need Clinical Language Models?
firstpage: 578
lastpage: 597
page: 578-597
order: 578
cycles: false
bibtex_author: eric, Lehman< and Hernandez, Evan and Mahajan, Diwakar and Wulff, Jonas
  and Smith, Micah J and Ziegler, Zachary and Nadler, Daniel and Szolovits, Peter
  and Johnson, Alistair and Alsentzer, Emily
author:
- given: Lehman<
  family: eric
- given: Evan
  family: Hernandez
- given: Diwakar
  family: Mahajan
- given: Jonas
  family: Wulff
- given: Micah J
  family: Smith
- given: Zachary
  family: Ziegler
- given: Daniel
  family: Nadler
- given: Peter
  family: Szolovits
- given: Alistair
  family: Johnson
- given: Emily
  family: Alsentzer
date: 2023-06-13
address:
container-title: Proceedings of the Conference on Health, Inference, and Learning
volume: '209'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 6
  - 13
pdf: https://proceedings.mlr.press/v209/eric23a/eric23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
